# Project File Structure & Explanation

This document provides a detailed breakdown of every file and folder in your project. Use this to explain your work to your professor.

## 1. Core Source Code

### `spark_pipeline.py`
**The Brain of the Project.** This is the main Python script that runs your entire Apache Spark pipeline.
*   **What it does:**
    1.  **Initializes Spark:** Sets up the Spark session with Delta Lake support and fixes local networking issues (binding to `127.0.0.1`).
    2.  **Bronze Layer (Ingestion):** Reads the raw `clickstream_data.json`, validates the schema, saves valid data to `delta/bronze`, and quarantines bad data to `delta/bad_records`.
    3.  **Silver Layer (Transformation):** Reads from Bronze, groups events by user, and applies **sessionization logic** (starts a new session after 30 minutes of inactivity). It saves this structured data to `delta/silver`.
    4.  **Gold Layer (Analytics):** Aggregates data to calculate metrics like session duration, page views, and conversion rates. It creates a "Funnel" (Home -> Search -> Product -> Cart -> Order) and saves the final report to `delta/gold` and `output/gold_report.csv`.
    5.  **Visualization:** Automatically generates `funnel_chart.png` and `session_duration.png` using Python plotting libraries.

### `generate_data.py`
**The Data Simulator.**
*   **What it does:** Generates synthetic (fake) clickstream data to simulate real users visiting an e-commerce website.
*   **Key Logic:** It creates random users, assigns them devices/browsers, and simulates a "journey" (e.g., Home -> Search -> Product -> Add to Cart). It introduces randomness in time intervals to make sessionization testing possible.
*   **Output:** `clickstream_data.json`.

### `pipeline.py`
**Legacy/Backup File.**
*   **What it does:** This file currently mirrors `spark_pipeline.py`. You can describe it as an alias or a backup entry point, but `spark_pipeline.py` is the primary script you are using.

### `visualize.py`
**Standalone Visualization Script.**
*   **What it does:** A separate script intended to create plots from the data.
*   **Note:** The plotting logic has been integrated directly into `spark_pipeline.py` for convenience, so this file is now optional but serves as a reference for how to use `matplotlib` and `seaborn` with Pandas.

---

## 2. Data & Output Folders

### `delta/` (Created at runtime)
**The Data Lakehouse Storage.** This folder contains your data stored in the **Delta Lake** format (Parquet files + Transaction Logs).
*   `delta/bronze`: Raw, immutable history of all valid events.
*   `delta/silver`: Cleaned data with Session IDs added.
*   `delta/gold`: Aggregated, business-level data (KPIs).
*   `delta/bad_records`: Malformed JSON records that failed validation.
*   `delta/intermediate`: Temporary data used during complex calculations.

### `output/`
**Human-Readable Exports.**
*   `gold_report.csv`: The final analytics report in CSV format (easy to open in Excel).
*   `bronze.csv` / `silver.csv`: Snapshots of the raw and intermediate layers for easy verification.

### `clickstream_data.json`
**Raw Input Data.** The file generated by `generate_data.py`. This simulates the stream of logs coming from a web server.

---

## 3. Documentation & Configuration

### `requirements.txt`
**Dependency List.**
*   **What it does:** Lists all the Python libraries your project needs (`pyspark`, `delta-spark`, `pandas`, `matplotlib`, `seaborn`, `setuptools`).
*   **Usage:** `pip install -r requirements.txt` installs everything at once.

### `CLI_Command.md`
**Cheat Sheet.**
*   **What it does:** Contains the exact terminal commands to set up the environment, generate data, and run the pipeline. Useful for quick reference.

### `README.md`
**Project Home Page.**
*   **What it does:** The standard entry point for anyone looking at your code. It explains *what* the project is, prerequisites, and how to run it.

### `Walkthrough.md` / `Walkthrough.pdf`
**Project Report.**
*   **What it does:** A step-by-step summary of what you accomplished. It includes proof of execution (screenshots/logs) and explains the results. The PDF is a professional export of the Markdown file.

### `task.md`
**Project Checklist.**
*   **What it does:** Tracks your progress. It lists all the requirements (Bronze/Silver/Gold layers, visualizations, etc.) and marks them as completed.

---

## 4. Environment

### `venv/`
**Virtual Environment.**
*   **What it does:** An isolated folder containing a specific version of Python and all the libraries from `requirements.txt`.
*   **Why use it?** It prevents conflicts with other Python projects on your computer. You "activate" it (`source venv/bin/activate`) to use the tools inside it.

---

## 5. Visualizations

### `funnel_chart.png`
**Conversion Funnel.**
*   **Shows:** How many sessions started at "Home" vs. how many made it to "Order Placed". This visualizes the drop-off rate.

### `session_duration.png`
**Histogram.**
*   **Shows:** The distribution of how long users spend on the site. Most sessions are likely short, with a "long tail" of highly engaged users.
